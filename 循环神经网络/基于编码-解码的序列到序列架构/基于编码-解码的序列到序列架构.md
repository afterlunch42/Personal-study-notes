### 1. 基于编码-解码的序列到序列架构

此处讨论如何训练RNN，使其将输入序列映射到**不一定等长的输出序列**。

经常将RNN的输入称为"**上下文**"，用$C$表示此上下文。$C$可能是一个丐萝输入序列$X=(x^{(1)},...,x^{(n_{x})})$的向量或向量序列。

![编码解码结构](.\img\编码解码结构.png)

该想法主要内容：

（1）**编码器**（encoder）或**读取器**（reader）或**输入**（input）RNN处理输入序列，编码器输出上下文$C$(通常是最终隐藏状态的简单函数)

（2）**解码器**（decoder）或**写入器**（writer）或**输出**（output）RNN则以固定长度的向量维为条件产生输出序列$\boldsymbol{Y}=(y^{(1)}, ...,y^{(n_y)})$。

此架构创新之处在于长度$n_x$和$n_y$可以彼此不同，而之前的架构约束$n_x=n_y=\tau$。

在序列到序列的架构中，两个RNN共同训练以最大化$logP(y^{(1)}, ...,y^{(n_y)}|x^{(1)},...,x^{(n_x)})$ （关于训练集中所有$x$, $y$对的平均）

向量到序列RNN至少有两种接受输入的方式。输入可以被提供为RNN的初始状态，或连接到每个时间步中的隐藏单元，且并不强制要求编码器与解码器是隐藏层具有相同的大小。

此架构的不足是：

编码器RNN输出的上下文$C$的维度太小而难以适当地概括一个长序列。为解决这个问题，可以通过一定方式将$C$称为可变长度的序列，而不是一个固定大小的向量，此外还可以引入将序列$C$的元素和输出序列的元素相关联的**注意力机制**(attention mechanism).

